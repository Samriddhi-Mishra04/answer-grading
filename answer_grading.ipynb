{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import fitz  # PyMuPDF\n",
        "import torch\n",
        "from transformers import BertTokenizer, BertModel\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "RMnKa3P-SEwi"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def upload_file():\n",
        "    \"\"\"Prompt user to enter a file path manually.\"\"\"\n",
        "    return input(\"Enter the file path: \")"
      ],
      "metadata": {
        "id": "3DSVi8KESd9p"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_text_from_pdf(pdf_path):\n",
        "    \"\"\"Extract text from a PDF file.\"\"\"\n",
        "    doc = fitz.open(pdf_path)\n",
        "    text = \"\\n\".join([page.get_text(\"text\") for page in doc])\n",
        "    return text if text.strip() else \"No text extracted\""
      ],
      "metadata": {
        "id": "m0hOm_FISilB"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_bert_embedding(text, model, tokenizer):\n",
        "    \"\"\"Generate BERT embeddings for the given text.\"\"\"\n",
        "    if not text.strip():\n",
        "        return np.zeros((1, 768))  # Return a zero vector if text is empty\n",
        "    inputs = tokenizer(text, return_tensors='pt', padding=True, truncation=True, max_length=512)\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "    return outputs.last_hidden_state.mean(dim=1).numpy()  # Get mean pooled embedding"
      ],
      "metadata": {
        "id": "z9MoaCQKSl0U"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def grade_answers():\n",
        "    \"\"\"Grade answers by comparing them with reference material.\"\"\"\n",
        "    tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "    model = BertModel.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "    print(\"Enter the reference PDF path\")\n",
        "    reference_pdf = upload_file()\n",
        "    print(\"Enter the answers PDF path\")\n",
        "    answers_pdf = upload_file()\n",
        "\n",
        "    reference_text = extract_text_from_pdf(reference_pdf).split(\"\\n\")\n",
        "    student_answers = extract_text_from_pdf(answers_pdf).split(\"\\n\")\n",
        "\n",
        "    if \"No text extracted\" in reference_text or not any(student_answers):\n",
        "        print(\"Error: One or both PDFs contain no extractable text.\")\n",
        "        return []\n",
        "\n",
        "    if len(reference_text) < 2:\n",
        "        print(\"Error: Reference PDF should contain at least two questions.\")\n",
        "        return []\n",
        "\n",
        "    reference_embeddings = [get_bert_embedding(q, model, tokenizer) for q in reference_text[:2]]\n",
        "    scores = []\n",
        "\n",
        "    for idx, ans in enumerate(student_answers[:2], start=1):  # Only compare the first two answers\n",
        "        if not ans.strip():\n",
        "            print(f\"Answer {idx}: No valid text detected.\")\n",
        "            scores.append(0)\n",
        "            continue\n",
        "\n",
        "        ans_embedding = get_bert_embedding(ans, model, tokenizer)\n",
        "        similarity = cosine_similarity(reference_embeddings[idx - 1], ans_embedding)\n",
        "        score = np.mean(similarity) * 100  # Convert similarity to percentage\n",
        "        scores.append(score)\n",
        "        print(f\"Answer {idx}: {score:.2f}% relevance\")\n",
        "\n",
        "    avg_score = np.mean(scores) if scores else 0\n",
        "    print(f\"\\nFinal Score: {avg_score:.2f}%\")\n",
        "\n",
        "    return scores"
      ],
      "metadata": {
        "id": "77MzAl_iSpk8"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example Usage\n",
        "if __name__ == \"__main__\":\n",
        "    scores = grade_answers()\n",
        "    print(\"Scores:\", scores)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OhRM6nz7SuIl",
        "outputId": "331e2cf1-6df8-42f8-846e-3ac5c66ee765"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter the reference PDF path\n",
            "Enter the file path: /content/answers.pdf\n",
            "Enter the answers PDF path\n",
            "Enter the file path: /content/student.pdf\n",
            "Answer 1: 76.48% relevance\n",
            "Answer 2: 35.81% relevance\n",
            "\n",
            "Final Score: 56.14%\n",
            "Scores: [76.47531032562256, 35.808539390563965]\n"
          ]
        }
      ]
    }
  ]
}