{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install PyMuPDF"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_byZbGybcbNM",
        "outputId": "66fbc8cf-aae7-4289-ea94-8bc774177dd4"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting PyMuPDF\n",
            "  Downloading pymupdf-1.25.3-cp39-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (3.4 kB)\n",
            "Downloading pymupdf-1.25.3-cp39-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (20.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.0/20.0 MB\u001b[0m \u001b[31m36.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: PyMuPDF\n",
            "Successfully installed PyMuPDF-1.25.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import fitz  # PyMuPDF\n",
        "import torch\n",
        "from transformers import BertTokenizer, BertModel\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np\n",
        "import spacy\n",
        "import re"
      ],
      "metadata": {
        "id": "r4bVx4YRh3Gd"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load spaCy's English model for sentence tokenization\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "def upload_file():\n",
        "    \"\"\"Prompt user to enter a file path manually.\"\"\"\n",
        "    return input(\"Enter the file path: \")\n",
        "\n",
        "def extract_text_from_pdf(pdf_path):\n",
        "    \"\"\"Extract text from a PDF file.\"\"\"\n",
        "    doc = fitz.open(pdf_path)\n",
        "    text = \"\\n\".join([page.get_text(\"text\") for page in doc])\n",
        "    return text if text.strip() else \"No text extracted\"\n",
        "\n",
        "def get_bert_embedding(text, model, tokenizer):\n",
        "    \"\"\"Generate BERT embeddings for the given text.\"\"\"\n",
        "    if not text.strip():\n",
        "        return np.zeros((1, 768))  # Return a zero vector if text is empty\n",
        "    inputs = tokenizer(text, return_tensors='pt', padding=True, truncation=True, max_length=512)\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "    return outputs.last_hidden_state.mean(dim=1).numpy()  # Get mean pooled embedding\n",
        "\n",
        "def sentence_similarity(reference_text, student_text, model, tokenizer):\n",
        "    \"\"\"Compare sentences between reference and student answers using BERT embeddings.\"\"\"\n",
        "    reference_sentences = [sent.text for sent in nlp(reference_text).sents]\n",
        "    student_sentences = [sent.text for sent in nlp(student_text).sents]\n",
        "\n",
        "    reference_embeddings = [get_bert_embedding(sentence, model, tokenizer) for sentence in reference_sentences]\n",
        "    student_embeddings = [get_bert_embedding(sentence, model, tokenizer) for sentence in student_sentences]\n",
        "\n",
        "    total_similarity = 0\n",
        "    matches = 0\n",
        "\n",
        "    for ref_embedding in reference_embeddings:\n",
        "        for student_embedding in student_embeddings:\n",
        "            similarity = cosine_similarity(ref_embedding, student_embedding)\n",
        "            if similarity > 0.75:  # Use a more strict threshold for high similarity\n",
        "                total_similarity += similarity\n",
        "                matches += 1\n",
        "\n",
        "    if matches == 0:\n",
        "        return 0  # No matches found, so score is 0\n",
        "\n",
        "    # Ensure total_similarity is a scalar before performing the division\n",
        "    return float(total_similarity) / matches * 100  # Return average similarity as percentage\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xmc3xlaWje4B",
        "outputId": "39bf2212-0cfa-42c4-edba-c87f203357b1"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/spacy/util.py:1740: UserWarning: [W111] Jupyter notebook detected: if using `prefer_gpu()` or `require_gpu()`, include it in the same cell right before `spacy.load()` to ensure that the model is loaded on the correct device. More information: http://spacy.io/usage/v3#jupyter-notebook-gpu\n",
            "  warnings.warn(Warnings.W111)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def point_wise_grading(reference_points, student_points):\n",
        "    \"\"\"Grading based on specific points in the reference answer.\"\"\"\n",
        "    total_points = len(reference_points)\n",
        "    if total_points == 0:  # Avoid division by zero\n",
        "        return 0  # Return 0 if no points are present in the reference answer\n",
        "\n",
        "    correct_points = 0\n",
        "\n",
        "    for i, ref_point in enumerate(reference_points):\n",
        "        if i < len(student_points):\n",
        "            student_point = student_points[i]\n",
        "            if ref_point.lower() in student_point.lower():  # Check for a partial match\n",
        "                correct_points += 1  # Full or partial match\n",
        "\n",
        "    return (correct_points / total_points) * 100  # Percentage of correct points\n",
        "\n",
        "def keyword_check(reference_answer, student_answer):\n",
        "    \"\"\"Check if student answer contains essential keywords from the reference answer.\"\"\"\n",
        "    reference_keywords = set(re.findall(r'\\w+', reference_answer.lower()))  # Extract words\n",
        "    student_keywords = set(re.findall(r'\\w+', student_answer.lower()))  # Extract words\n",
        "    common_keywords = reference_keywords.intersection(student_keywords)\n",
        "    return len(common_keywords) / len(reference_keywords)  # Ratio of common words to total reference words\n",
        "\n",
        "def grade_answers():\n",
        "    \"\"\"Grade answers by comparing them with reference material.\"\"\"\n",
        "    tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "    model = BertModel.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "    print(\"Enter the reference PDF path (correct answers PDF)\")\n",
        "    reference_pdf = upload_file()\n",
        "    print(\"Enter the answers PDF path (student answers PDF)\")\n",
        "    answers_pdf = upload_file()\n",
        "\n",
        "    reference_text = extract_text_from_pdf(reference_pdf)\n",
        "    student_answers = extract_text_from_pdf(answers_pdf)\n",
        "\n",
        "    if \"No text extracted\" in reference_text or not student_answers.strip():\n",
        "        print(\"Error: One or both PDFs contain no extractable text.\")\n",
        "        return []\n",
        "\n",
        "    # Split reference and student answers into individual answers (A1, A2)\n",
        "    reference_answers = reference_text.split(\"\\n\")[:2]  # Assuming there are two reference answers\n",
        "    student_answers = student_answers.split(\"\\n\")[:2]  # Assuming there are two student answers\n",
        "\n",
        "    # Split each answer into the basic explanation and numbered points\n",
        "    def split_answer(answer):\n",
        "        basic_explanation = answer.split(\"\\n\")[0]  # First line is the basic explanation\n",
        "        points = [point.strip() for point in answer.split(\"\\n\")[1:] if point.strip()]  # Rest are points\n",
        "        return basic_explanation, points\n",
        "\n",
        "    ref_explanation_A1, ref_points_A1 = split_answer(reference_answers[0])\n",
        "    student_explanation_A1, student_points_A1 = split_answer(student_answers[0])\n",
        "\n",
        "    ref_explanation_A2, ref_points_A2 = split_answer(reference_answers[1])\n",
        "    student_explanation_A2, student_points_A2 = split_answer(student_answers[1])\n",
        "\n",
        "    # Grading the explanation (basic explanation comparison)\n",
        "    print(\"Grading Answer 1 (A1) Explanation...\")\n",
        "    score_A1_explanation = sentence_similarity(ref_explanation_A1, student_explanation_A1, model, tokenizer)\n",
        "    print(f\"Answer 1 Explanation Score: {score_A1_explanation:.2f}%\")\n",
        "\n",
        "    print(\"Grading Answer 2 (A2) Explanation...\")\n",
        "    score_A2_explanation = sentence_similarity(ref_explanation_A2, student_explanation_A2, model, tokenizer)\n",
        "    print(f\"Answer 2 Explanation Score: {score_A2_explanation:.2f}%\")\n",
        "\n",
        "    # Point-wise grading for A1 and A2\n",
        "    print(\"Grading Answer 1 (A1) Points...\")\n",
        "    score_A1_points = point_wise_grading(ref_points_A1, student_points_A1)\n",
        "    print(f\"Answer 1 Points Score: {score_A1_points:.2f}%\")\n",
        "\n",
        "    print(\"Grading Answer 2 (A2) Points...\")\n",
        "    score_A2_points = point_wise_grading(ref_points_A2, student_points_A2)\n",
        "    print(f\"Answer 2 Points Score: {score_A2_points:.2f}%\")\n",
        "\n",
        "    # Combine explanation and point scores for each answer\n",
        "    total_score_A1 = (score_A1_explanation + score_A1_points) / 2\n",
        "    total_score_A2 = (score_A2_explanation + score_A2_points) / 2\n",
        "\n",
        "    print(f\"Total Score for Answer 1: {total_score_A1:.2f}%\")\n",
        "    print(f\"Total Score for Answer 2: {total_score_A2:.2f}%\")\n",
        "\n",
        "    # Calculate the final average score\n",
        "    final_score = (total_score_A1 + total_score_A2) / 2\n",
        "    print(f\"Final Average Score: {final_score:.2f}%\")\n",
        "\n",
        "    return final_score\n",
        "\n",
        "# Example Usage\n",
        "if __name__ == \"__main__\":\n",
        "    score = grade_answers()\n",
        "    print(\"Average Score:\", score)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OPTTr0Xjjn-T",
        "outputId": "a7f9fd0f-84be-4cdd-c628-68c27d435f63"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter the reference PDF path (correct answers PDF)\n",
            "Enter the file path: /content/reference.pdf\n",
            "Enter the answers PDF path (student answers PDF)\n",
            "Enter the file path: /content/answers.pdf\n",
            "Grading Answer 1 (A1) Explanation...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-24-31762b20c060>:45: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
            "  return float(total_similarity) / matches * 100  # Return average similarity as percentage\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer 1 Explanation Score: 100.00%\n",
            "Grading Answer 2 (A2) Explanation...\n",
            "Answer 2 Explanation Score: 98.21%\n",
            "Grading Answer 1 (A1) Points...\n",
            "Answer 1 Points Score: 0.00%\n",
            "Grading Answer 2 (A2) Points...\n",
            "Answer 2 Points Score: 0.00%\n",
            "Total Score for Answer 1: 50.00%\n",
            "Total Score for Answer 2: 49.10%\n",
            "Final Average Score: 49.55%\n",
            "Average Score: 49.55212622880936\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-24-31762b20c060>:45: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
            "  return float(total_similarity) / matches * 100  # Return average similarity as percentage\n"
          ]
        }
      ]
    }
  ]
}